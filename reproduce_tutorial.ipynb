{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa8f9ece",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "584f1eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred: name 'Yes' is not defined\n",
      "['Who was the oldest justice on the United States Supreme Court in 1980?']\n",
      "['When was Justice William O. Douglas born?']\n",
      "['How long did Justice William O. Douglas serve on the Supreme Court?', 'When did Justice William O. Douglas retire from the Supreme Court?']\n",
      "['When did Justice Douglas die?']\n",
      "['Who was the oldest serving justice on the Court at that time?', 'Who was Justice Douglas and when did he serve on the Court?']\n",
      "An unexpected error occurred: name 'true' is not defined.\n",
      "An unexpected error occurred: name 'Yes' is not defined\n",
      "An unexpected error occurred: name 'Yes' is not defined\n",
      "['When did Barack Obama serve as president?', 'When did Donald Trump serve as president?', 'Who were the black presidents of the United States?', 'How many black presidents has the United States had?']\n",
      "['Who was the first black president of the United States?']\n",
      "['Where is Honolulu, Hawaii located?', 'Where is Kansas located?', 'Where was Barack Obama born?', \"Where is Barack Obama's father from?\", 'Where is Kenya located?', \"Where is Barack Obama's mother from?\"]\n",
      "An unexpected error occurred: name 'true' is not defined.\n",
      "An unexpected error occurred: name 'true' is not defined.\n",
      "An unexpected error occurred: name 'true' is not defined.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./src\")\n",
    "\n",
    "import pandas as pd\n",
    "from decompose import doc2sentences\n",
    "from checkworthy import identify_checkworthiness, specify_checkworthiness_type\n",
    "from retrieve import get_web_evidences_for_claim\n",
    "from verify import verify_claim\n",
    "from pipeline import check_document, check_documents\n",
    "\n",
    "labels = check_documents(\"./test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "434421cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f113be",
   "metadata": {},
   "source": [
    "### Subtask 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b9a531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e59737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\apps\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\apps\\miniconda3\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "No sentence-transformers model found with name C:\\Users\\Yuxia.wang/.cache\\torch\\sentence_transformers\\princeton-nlp_sup-simcse-roberta-large. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from baselines import evaluate_revisions, subtask5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e38f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtask5(datadir=\"./test/subtask5_revision.jsonl\", savedir=\"./test/result/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb1daa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Index(['factual state', 'claim list', 'prompt', 'response', 'revised_response',\n",
      "       'with-question-gpt-3.5', 'with-question-gpt-4', 'no-question-gpt-3.5',\n",
      "       'no-question-gpt-4'],\n",
      "      dtype='object')\n",
      "with-question-gpt-3.5 \n",
      "\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edit_distance 51.5\n",
      "norm_edit_distance 0.175\n",
      "norm_edit_similarity 0.825\n",
      "bigram_distance 0.176\n",
      "word_overlap 0.885\n",
      "precision 0.963\n",
      "recall 0.978\n",
      "f1 0.97\n",
      "cosine-sentence-roberta 0.868\n",
      "with-question-gpt-4 \n",
      "\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edit_distance 106.0\n",
      "norm_edit_distance 0.251\n",
      "norm_edit_similarity 0.749\n",
      "bigram_distance 0.254\n",
      "word_overlap 0.84\n",
      "precision 0.953\n",
      "recall 0.97\n",
      "f1 0.962\n",
      "cosine-sentence-roberta 0.844\n",
      "no-question-gpt-3.5 \n",
      "\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edit_distance 61.5\n",
      "norm_edit_distance 0.211\n",
      "norm_edit_similarity 0.789\n",
      "bigram_distance 0.213\n",
      "word_overlap 0.884\n",
      "precision 0.961\n",
      "recall 0.978\n",
      "f1 0.969\n",
      "cosine-sentence-roberta 0.866\n",
      "no-question-gpt-4 \n",
      "\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edit_distance 107.25\n",
      "norm_edit_distance 0.251\n",
      "norm_edit_similarity 0.749\n",
      "bigram_distance 0.254\n",
      "word_overlap 0.846\n",
      "precision 0.953\n",
      "recall 0.971\n",
      "f1 0.962\n",
      "cosine-sentence-roberta 0.844\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_revisions(datadir=\"./test/result/st5.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d95ba6",
   "metadata": {},
   "source": [
    "#### Human Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "547d29f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ['no-question-gpt-4', 'with-question-gpt-4']\n",
      "42 ['no-question-gpt-4', 'with-question-gpt-4']\n",
      "45 ['no-question-gpt-3.5', 'no-question-gpt-4', 'with-question-gpt-3.5', 'with-question-gpt-4']\n",
      "66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'no-question-gpt-4': 28,\n",
       "         'with-question-gpt-4': 15,\n",
       "         'with-question-gpt-3.5': 13,\n",
       "         'no-question-gpt-3.5': 10})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "unshuffledir = \"./subtasks_data/result/st5_human_eval/\"\n",
    "\n",
    "human_prefer = []\n",
    "for i in range(61):\n",
    "    temp = pd.read_json(os.path.join(unshuffledir, f\"{i}.json\"), typ=\"series\")\n",
    "    hls = temp[\"human_preference\"].split(\",\")\n",
    "    # for some examples, preference is more than one, separated by comma\n",
    "    if len(hls) > 1:\n",
    "        hls = [s.strip() for s in hls]\n",
    "        print(i, hls)\n",
    "    human_prefer += hls\n",
    "print(len(human_prefer))\n",
    "Counter(human_prefer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa119df",
   "metadata": {},
   "source": [
    "### Subtask 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aafadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assess Factool: https://github.com/GAIR-NLP/factool.git in subtask 4;\n",
    "# Web source setting serper or our pipeline retriever with Google search results\n",
    "# Wikipedia setting: calling the Factscore retriever based on wikipedia database\n",
    "\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"set_openai_key\"\n",
    "os.environ['SERPAPI_API_KEY'] = \"set_serpapi_key\"\n",
    "from factool.knowledge_qa.pipeline import knowledge_qa_pipeline\n",
    "\n",
    "def run_subtask4(claim):\n",
    "    foundation_model = \"gpt-3.5-turbo\"\n",
    "    kbqa_pipeline = knowledge_qa_pipeline(foundation_model, 10, \"online\")\n",
    "    claims = [{\"claim\": claim}]\n",
    "    output = asyncio.run(kbqa_pipeline.run_with_tool_live_without_claim_extraction(claims))\n",
    "    return output[0][\"factuality\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92382e3b",
   "metadata": {},
   "source": [
    "### Subtask 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a35005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0544dcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\apps\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\apps\\miniconda3\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from baselines import nli_predict_stance, eval_subtask3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "915edef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load cached predictions from ./subtasks_data/result/st3_roberta_mnli.json...\n"
     ]
    }
   ],
   "source": [
    "gold, predictions, metrics = \\\n",
    "nli_predict_stance(datadir=\"./subtasks_data/subtask3_claim_evidence_stance.jsonl\", \n",
    "                   savedir=\"./subtasks_data/result/st3_roberta_mnli.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e38b36c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.7142857142857143,\n",
       "  'recall': 0.5197792088316467,\n",
       "  'f1-score': 0.6017039403620874,\n",
       "  'support': 1087},\n",
       " '2': {'precision': 0.14661134163208853,\n",
       "  'recall': 0.6583850931677019,\n",
       "  'f1-score': 0.2398190045248869,\n",
       "  'support': 161},\n",
       " '3': {'precision': 0.7459519821328866,\n",
       "  'recall': 0.6494895478852698,\n",
       "  'f1-score': 0.6943866943866943,\n",
       "  'support': 2057},\n",
       " 'accuracy': 0.6072617246596067,\n",
       " 'macro avg': {'precision': 0.5356163460168964,\n",
       "  'recall': 0.6092179499615394,\n",
       "  'f1-score': 0.5119698797578895,\n",
       "  'support': 3305},\n",
       " 'weighted avg': {'precision': 0.7063407638967278,\n",
       "  'recall': 0.6072617246596067,\n",
       "  'f1-score': 0.6417599011363164,\n",
       "  'support': 3305}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b082e666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load cached predictions from ./subtasks_data/result/st3_gpt3.5_zs.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yuxia.wang\\Desktop\\Document-level-fact-checking\\./src\\baselines.py:230: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for k, v in responses.iteritems():\n"
     ]
    }
   ],
   "source": [
    "gold, preds, metrics = \\\n",
    "eval_subtask3(datadir=\"./subtasks_data/subtask3_claim_evidence_stance.jsonl\", \n",
    "              response_savedir=\"./subtasks_data/result/st3_gpt3.5_zs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c55af2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.506006006006006,\n",
       "  'recall': 0.48419540229885055,\n",
       "  'f1-score': 0.49486049926578557,\n",
       "  'support': 696},\n",
       " '2': {'precision': 0.14927048260381592,\n",
       "  'recall': 0.340153452685422,\n",
       "  'f1-score': 0.20748829953198128,\n",
       "  'support': 391},\n",
       " '3': {'precision': 0.0998003992015968,\n",
       "  'recall': 0.6211180124223602,\n",
       "  'f1-score': 0.17196904557179707,\n",
       "  'support': 161},\n",
       " '4': {'precision': 0.853887399463807,\n",
       "  'recall': 0.30967428293631505,\n",
       "  'f1-score': 0.4545130217623975,\n",
       "  'support': 2057},\n",
       " 'accuracy': 0.36520423600605145,\n",
       " 'macro avg': {'precision': 0.4022410718188064,\n",
       "  'recall': 0.438785287585737,\n",
       "  'f1-score': 0.3322077165329903,\n",
       "  'support': 3305},\n",
       " 'weighted avg': {'precision': 0.6605322795300395,\n",
       "  'recall': 0.36520423600605145,\n",
       "  'f1-score': 0.4200215233610597,\n",
       "  'support': 3305}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d25a1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'label': 'ENTAILMENT', 'score': 0.48039454221725464}}\n"
     ]
    }
   ],
   "source": [
    "gold, predictions, metrics = \\\n",
    "nli_predict_stance(datadir=\"./test/subtask3_claim_evidence_stance.jsonl\", \n",
    "                   savedir=\"./test/result/st3_roberta_mnli.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5600b744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.8,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.888888888888889,\n",
       "  'support': 4},\n",
       " '2': {'precision': 0.3333333333333333,\n",
       "  'recall': 0.5,\n",
       "  'f1-score': 0.4,\n",
       "  'support': 2},\n",
       " '3': {'precision': 1.0,\n",
       "  'recall': 0.3333333333333333,\n",
       "  'f1-score': 0.5,\n",
       "  'support': 3},\n",
       " 'accuracy': 0.6666666666666666,\n",
       " 'macro avg': {'precision': 0.7111111111111111,\n",
       "  'recall': 0.611111111111111,\n",
       "  'f1-score': 0.5962962962962963,\n",
       "  'support': 9},\n",
       " 'weighted avg': {'precision': 0.7629629629629631,\n",
       "  'recall': 0.6666666666666666,\n",
       "  'f1-score': 0.6506172839506174,\n",
       "  'support': 9}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c079e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load cached predictions from ./test/result/st3_gpt3.5_zs.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yuxia.wang\\Desktop\\Document-level-fact-checking\\./src\\baselines.py:230: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for k, v in responses.iteritems():\n",
      "C:\\apps\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\apps\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\apps\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "gold, preds, metrics = \\\n",
    "eval_subtask3(datadir=\"./test/subtask3_claim_evidence_stance.jsonl\", \n",
    "              response_savedir=\"./test/result/st3_gpt3.5_zs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d92475c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.5,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.6666666666666666,\n",
       "  'support': 3},\n",
       " '2': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1},\n",
       " '3': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2},\n",
       " '4': {'precision': 1.0,\n",
       "  'recall': 0.3333333333333333,\n",
       "  'f1-score': 0.5,\n",
       "  'support': 3},\n",
       " 'accuracy': 0.6666666666666666,\n",
       " 'macro avg': {'precision': 0.625,\n",
       "  'recall': 0.5833333333333334,\n",
       "  'f1-score': 0.5416666666666666,\n",
       "  'support': 9},\n",
       " 'weighted avg': {'precision': 0.7222222222222222,\n",
       "  'recall': 0.6666666666666666,\n",
       "  'f1-score': 0.6111111111111112,\n",
       "  'support': 9}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827dc5bc",
   "metadata": {},
   "source": [
    "### Subtask 1 and 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0e88ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\apps\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\apps\\miniconda3\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from baselines import eval_sentence_checkworthiness, eval_claim_checkworthiness, all_checkworthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e18305c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load cached predictions from ./subtasks_data/result/st1_gpt3.5_zs.json...\n",
      "Load cached predictions from ./subtasks_data/result/st2_gpt3.5_zs.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\apps\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\apps\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\apps\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.98125,\n",
       "  'recall': 0.9500756429652042,\n",
       "  'f1-score': 0.9654112221368177,\n",
       "  'support': 661},\n",
       " '2': {'precision': 0.21428571428571427,\n",
       "  'recall': 0.1875,\n",
       "  'f1-score': 0.19999999999999998,\n",
       "  'support': 16},\n",
       " '3': {'precision': 0.058823529411764705,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.1111111111111111,\n",
       "  'support': 1},\n",
       " '4': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n",
       " 'accuracy': 0.9321533923303835,\n",
       " 'macro avg': {'precision': 0.3135898109243697,\n",
       "  'recall': 0.534393910741301,\n",
       "  'f1-score': 0.31913058331198224,\n",
       "  'support': 678},\n",
       " 'weighted avg': {'precision': 0.9617900368111845,\n",
       "  'recall': 0.9321533923303835,\n",
       "  'f1-score': 0.9460883907721943,\n",
       "  'support': 678}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, gold, metrics = eval_sentence_checkworthiness(\n",
    "    datadir=\"./subtasks_data/subtask1_sentence_checkworthiness.jsonl\",\n",
    "    response_savedir=\"./subtasks_data/result/st1_gpt3.5_zs.json\")\n",
    "\n",
    "\n",
    "preds, gold, metrics = eval_claim_checkworthiness(\n",
    "    datadir=\"./subtasks_data/subtask2_claim_checkworthiness.jsonl\",\n",
    "    response_savedir=\"./subtasks_data/result/st2_gpt3.5_zs.json\")\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a5cd752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 34}, '1': {'precision': 0.8906752411575563, 'recall': 1.0, 'f1-score': 0.9421768707482993, 'support': 277}, 'accuracy': 0.8906752411575563, 'macro avg': {'precision': 0.4453376205787781, 'recall': 0.5, 'f1-score': 0.47108843537414963, 'support': 311}, 'weighted avg': {'precision': 0.793302385211071, 'recall': 0.8906752411575563, 'f1-score': 0.8391736115668132, 'support': 311}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\apps\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\apps\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\apps\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "all_checkworthy(datadir = \"./subtasks_data/subtask1_sentence_checkworthiness.jsonl\", \n",
    "                granularity=\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ab602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'precision': 0.9749262536873157, 'recall': 1.0, 'f1-score': 0.9873039581777446, 'support': 661}, '2': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 16}, '3': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'accuracy': 0.9749262536873157, 'macro avg': {'precision': 0.3249754178957719, 'recall': 0.3333333333333333, 'f1-score': 0.3291013193925815, 'support': 678}, 'weighted avg': {'precision': 0.9504812001287842, 'recall': 0.9749262536873157, 'f1-score': 0.9625485491968868, 'support': 678}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\apps\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\apps\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\apps\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "all_checkworthy(datadir = \"./subtasks_data/subtask2_claim_checkworthiness.jsonl\", \n",
    "                granularity=\"claim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3718ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4971bddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f600f684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
